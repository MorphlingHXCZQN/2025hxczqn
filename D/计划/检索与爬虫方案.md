# 食道肿瘤影像文献检索与爬虫方案

## 1. 检索目标与范围
- **研究主题**：食道肿瘤/食道癌 + 影像学（MRI、CT、cT分期、影像组学等）。
- **时间范围**：最近 5 年（从当前年份向前滚动）。
- **数据源**：PubMed、Google Scholar（必要时辅以 Crossref、DOI.org）。
- **输出要求**：
  - 可公开下载的开源全文 PDF 或者补全摘要与引用元数据。
  - 后续总结所需的关键信息字段（研究设计、样本量、数据类型、模型/算法、主要结论）。
  - 落盘路径默认使用配置文件中的 `output_root`，如在 Windows 本地需写入 `D:\计划\`，执行时追加 `--output-root "D:/计划"`。

## 2. 关键字设计
1. **基础词表**
   - 疾病相关："esophageal cancer", "esophageal neoplasms", "esophageal squamous cell carcinoma", "esophageal adenocarcinoma"。
   - 影像相关："MRI", "magnetic resonance", "cT staging", "computed tomography", "radiomics", "deep learning", "machine learning"。
   - 其他限制："2020:2024"（PubMed 的 `dp` 字段），"last 5 years"（Google Scholar 过滤）。
2. **布尔逻辑示例**
   - PubMed：`("esophageal neoplasms"[MeSH Terms] OR "esophageal cancer"[Title/Abstract]) AND ("magnetic resonance imaging"[MeSH Terms] OR MRI[Title/Abstract] OR "computed tomography"[Title/Abstract] OR radiomics[Title/Abstract]) AND ("2019/01/01"[Date - Publication] : "3000"[Date - Publication])`。
   - Google Scholar：`"esophageal cancer" (MRI OR "computed tomography" OR radiomics) (cT OR staging) since 2019`。
3. **迭代策略**
   - 结合自动查询与人工复核，对高频词和特定方法（如 radiomics、deep learning）做二次检索。
   - 针对特定疗法（术前放化疗）做补充查询，确保覆盖数据背景。

## 3. 爬虫与数据采集工作流
```
+-----------------+
| 关键词配置 YAML |
+-----------------+
          |
          v
+------------------+        +--------------------+
| 查询调度器 (Python) |----->| PubMed API 采集模块 |
+------------------+        +--------------------+
          |                          |
          |                          v
          |                 +------------------+
          |                 | Google Scholar   |
          |                 | 抓取模块         |
          |                 +------------------+
          |                          |
          v                          v
+------------------+        +--------------------+
| 元数据清洗/去重   |<------| 数据落盘 (JSON/CSV) |
+------------------+        +--------------------+
          |
          v
+-----------------------+
| PDF 下载与存档        |
+-----------------------+
          |
          v
+-----------------------+
| 文献总结与引用输出    |
+-----------------------+
```

## 4. PubMed API 采集细节
- **接口**：NCBI E-utilities (`esearch`, `efetch`, `esummary`)；使用 `Biopython` 的 `Bio.Entrez`。
- **步骤**：
  1. `esearch` 获取 PMID 列表，启用 `retmax` 控制批量（建议 100 条/批）。
  2. `efetch` 下载 XML 或 JSON，提取题目、摘要、作者、期刊、DOI、出版年等字段。
  3. 若 `ArticleIdList` 包含 DOI，记录以便后续通过 Crossref/PDFHub 下载全文。
- **限速**：设置 `Entrez.email` 与 `api_key`，遵守每秒不超过 10 次请求；使用 `time.sleep(0.34)`。
- **错误处理**：
  - 网络异常时指数退避重试（上限 3 次）。
  - 若无摘要，则标记 `abstract_missing`，供人工补录。

## 5. Google Scholar 抓取方案
> 注意：Google Scholar 服务条款禁止未经授权的大规模抓取，应尽量使用官方/授权 API。以下提供合规优先的方案与备选。

1. **优先方案：第三方授权 API**
   - 例如 SerpAPI、Publish or Perish API，支持 `engine=google_scholar`。
   - 配置查询参数：`q`, `as_ylo=2019`, `num=20`, `start` 分页。
   - 返回 JSON，可直接提取标题、作者、发表年份、引用数及 PDF 链接。
2. **备选方案：学术库镜像/学术搜索包**
   - 使用 `scholarly`（需配置 `TOR` 或代理，遵守访问频率 < 1 请求/5 秒）。
   - 仅抓取元数据，不直接下载非开源 PDF，保留引用链接。
3. **人工复核**
   - 对于 PDF 链接指向出版社会员页面的，标记 `access="restricted"`，只保留摘要信息。

## 6. PDF 下载与存储
- **文件结构**：
  - `D/计划/data/raw/pubmed/`：存放原始 JSON/XML。
  - `D/计划/data/raw/google_scholar/`：存放 Google Scholar JSON/CSV。
  - `D/计划/data/pdfs/open_access/`：保存下载成功的开源 PDF。
  - `D/计划/data/pdfs/restricted/`：存放引用信息（无 PDF）。
- **命名规范**：`<年份>_<一作姓>_<期刊或会议>_<短标题>.pdf`，特殊字符用下划线替换。
- **校验**：下载后计算 SHA-256，存入 `metadata.json` 以便去重。

## 7. 元数据清洗与汇总
- 使用 `pipeline/search.py` + `pipeline/downloader.py` 的组合在离线环境中完成字段对齐，写入 `data/processed/literature_summary.csv`。
- 字段覆盖：`title`, `authors`, `year`, `journal`, `doi`, `study_type`, `sample_size`, `imaging_modality`, `key_findings`, `access_type`, `citations`。
- 去重规则：优先 DOI，其次题目字符串完全匹配；可在接入真实数据时扩展模糊匹配或引用数校正。

## 8. 文献总结与报告自动化
- 对可下载的 PDF 使用 `GROBID` 或 `Science Parse` 解析结构化文本。
- 利用现有 `LLM` 工具离线总结时，需先本地提取段落，限定输入长度。
- 摘要模板（位于 `literature_review_template.md`，如需 Word 版可手动导出）：
  - 研究背景
  - 数据来源与规模
  - 模型/算法
  - 主要结论
  - 局限性与下一步建议
- 汇总结果写入 `D/计划/文献自动总结.md`（自动生成示例）或运行管线获取的 `文献自动总结.docx`（运行时生成，不纳入版本控制），并更新 `research_plan_draft.md` 中的引用。
- `pipeline/iteration.py` 自动输出 7 轮研究迭代记录（`研究方案迭代日志.md`）与定稿方案（`研究方案最终稿.md`），供团队讨论及伦理材料准备。

## 9. 自动化脚本建议
- 主脚本 `python -m pipeline.run --config configs/search.yml`，核心模块：
  - `pipeline/search.py`：关键词组合与任务队列。
  - `pipeline/pubmed.py`：封装 Entrez 调用。
  - `pipeline/google_scholar.py`：封装 SerpAPI/Scholarly。
  - `pipeline/downloader.py`：PDF/摘要下载与存储。
  - `pipeline/summarizer.py`：结合 NLP/LLM 生成要点。
  - `pipeline/iteration.py`：基于高引用文献形成 7 轮迭代与最终方案。
- 所有脚本的输出与日志均写入 `D/计划/logs/`。

## 10. 合规与维护
- 记录每次批量检索的查询参数与时间戳，保存到 `D/计划/logs/search_history.csv`。
- 若需共享 PDF，确认版权状态：遵循 `open access`、作者许可或机构订阅范围。
- 每季度回顾关键词并更新配置，保证覆盖新兴研究方向（如多模态融合、放疗响应预测等）。

